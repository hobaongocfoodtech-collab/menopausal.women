import pandas as pd
import numpy as np
import os
import warnings
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.preprocessing import StandardScaler

warnings.filterwarnings('ignore')

# --- C·∫§U H√åNH ---
# S·ª≠ d·ª•ng t·∫≠p Train ƒë√£ ƒë∆∞·ª£c c√¢n b·∫±ng b·∫±ng SMOTE
INPUT_TRAIN = "train_data_balanced.csv"
INPUT_TEST = "test_data.csv"

# Danh s√°ch 25 c√¢u h·ªèi ti·ªÅm nƒÉng (ƒê√£ lo·∫°i b·ªè Chronic_Disease v√¨ ƒë∆∞a v√†o Demo Feats)
POTENTIAL_QUESTIONS = [
    'MEN_HotFlash', 'MEN_Memory', 'MEN_Sleep', 'MEN_Headache', 'MEN_MuscleAche',
    'MEN_Fatigue', 'MEN_Weight', 'MEN_Skin', 'MEN_Depressed', 'MEN_Impatient',
    'MEN_Urine', 'MEN_Libido', 'PSS_1', 'PSS_2', 'PSS_3', 'PSS_4', 'PSS_5',
    'PSS_6', 'PSS_7', 'PSS_8', 'PSS_9', 'PSS_10', 'Supp_Calcium', 'Supp_Omega3'
]

# Th√™m Chronic_Disease v√†o nh√≥m ƒë·∫∑c tr∆∞ng n·ªÅn t·∫£ng
DEMO_FEATS = ['Age', 'BMI', 'Education_Code', 'Income_Code', 'Job_Code', 'Marital_Code',
              'Meno_Duration_New', 'Meno_Status', 'Chronic_Disease']


def run_fine_tuning():
    if not os.path.exists(INPUT_TRAIN):
        print(f"‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y file '{INPUT_TRAIN}'. H√£y ch·∫°y b∆∞·ªõc 3.5 tr∆∞·ªõc.")
        return

    train_df = pd.read_csv(INPUT_TRAIN)
    test_df = pd.read_csv(INPUT_TEST)

    targets = ['PSS_Score', 'MENQOL_Score']
    scaler = StandardScaler()

    print("--- GIAI ƒêO·∫†N 5: T√åM C√ÇU H·ªéI V√ÄNG & FINE-TUNING (SMOTE ENHANCED) ---")

    for target in targets:
        print(f"\n" + "=" * 60)
        print(f"üöÄ T·ªêI ∆ØU H√ìA M·ª§C TI√äU: {target}")
        print("=" * 60)

        for c_id in sorted(train_df['Cluster'].unique()):
            c_train = train_df[train_df['Cluster'] == c_id]
            c_test = test_df[test_df['Cluster'] == c_id]

            # --- B∆Ø·ªöC 1: T√åM C√ÇU H·ªéI V√ÄNG (FEATURE IMPORTANCE) ---
            # S·ª≠ d·ª•ng t·∫≠p Train ƒë√£ c√¢n b·∫±ng gi√∫p vi·ªác t√¨m feature importance ch√≠nh x√°c h∆°n
            selector = ExtraTreesRegressor(n_estimators=100, random_state=42)
            valid_potential = [q for q in POTENTIAL_QUESTIONS if q in c_train.columns]

            selector.fit(c_train[valid_potential], c_train[target])
            importances = pd.Series(selector.feature_importances_, index=valid_potential)
            gold_qs = importances.nlargest(5).index.tolist()

            print(f"\nüìç Cluster {c_id} (Train: {len(c_train)} m·∫´u | Test: {len(c_test)} m·∫´u):")
            print(f"   -> 5 C√¢u h·ªèi v√†ng: {gold_qs}")

            # --- B∆Ø·ªöC 2: FINE-TUNING V·ªöI GRID SEARCH ---
            final_features = DEMO_FEATS + gold_qs
            X_train = c_train[final_features]
            y_train = c_train[target]

            # Chu·∫©n h√≥a d·ªØ li·ªáu tr∆∞·ªõc khi train
            X_train_scaled = scaler.fit_transform(X_train)

            param_grid = {
                'n_estimators': [100, 300],
                'max_depth': [4, 6, 8, None],
                'min_samples_leaf': [2, 4],
                'max_features': ['sqrt', 'log2', None]
            }

            grid = GridSearchCV(ExtraTreesRegressor(random_state=42), param_grid, cv=3, scoring='r2')
            grid.fit(X_train_scaled, y_train)

            best_model = grid.best_estimator_
            print(f"   -> Best Params: {grid.best_params_}")

            # --- B∆Ø·ªöC 3: ƒê√ÅNH GI√Å TR√äN T·∫¨P TEST TH·∫¨T ---
            if len(c_test) > 0:
                X_test_scaled = scaler.transform(c_test[final_features])
                y_test = c_test[target]
                y_pred = best_model.predict(X_test_scaled)

                # T√≠nh R2 v√† MAE
                # N·∫øu ch·ªâ c√≥ 1 m·∫´u, R2 s·∫Ω l√† nan, ta c·∫ßn check ƒë·ªÉ tr√°nh in ra nan
                if len(y_test) > 1:
                    r2 = r2_score(y_test, y_pred)
                    print(f"   ‚úÖ R2 tr√™n t·∫≠p TEST: {r2:.4f}")
                else:
                    print(f"   ‚ö†Ô∏è Ch·ªâ c√≥ 1 m·∫´u test: Th·ª±c t·∫ø={y_test.values[0]:.2f}, D·ª± b√°o={y_pred[0]:.2f}")

                mae = mean_absolute_error(y_test, y_pred)
                print(f"   ‚úÖ MAE tr√™n t·∫≠p TEST: {mae:.4f}")
            else:
                print("   ‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu test cho c·ª•m n√†y.")


if __name__ == "__main__":
    run_fine_tuning()