import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_absolute_error
import warnings
import os

warnings.filterwarnings('ignore')

# 1. C·∫§U H√åNH D·ªÆ LI·ªÜU
FILE_PATH = r"/menopause_qol\data\processed\clean_data_final.csv"

# --- ƒê·ªäNH NGHƒ®A NH√ìM BI·∫æN ---
DEMO_FEATS = ['Age', 'BMI', 'Education_Code', 'Income_Code', 'Marital_Code',
              'Job_Code', 'Info_Search_Code', 'Meno_Duration', 'Meno_Group']

# Danh s√°ch c√°c c√¢u h·ªèi ti·ªÅm nƒÉng
POTENTIAL_FEATS = [
    'MEN_HotFlash', 'MEN_Memory', 'MEN_Sleep', 'MEN_Headache', 'MEN_MuscleAche',
    'MEN_Fatigue', 'MEN_Weight', 'MEN_Skin', 'MEN_Depressed', 'MEN_Impatient',
    'MEN_Urine', 'MEN_Libido',
    'PSS_1', 'PSS_2', 'PSS_3', 'PSS_4', 'PSS_5', 'PSS_6', 'PSS_7', 'PSS_8', 'PSS_9', 'PSS_10',
    'Chronic_Disease', 'Supp_Calcium', 'Supp_Omega3', 'Exercise'
]

def run_adaptive_pipeline():
    if not os.path.exists(FILE_PATH):
        print(f"‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y file {FILE_PATH}")
        return

    df = pd.read_csv(FILE_PATH)

    # --- B∆Ø·ªöC S·ª¨A L·ªñI: M√É H√ìA D·ªÆ LI·ªÜU D·∫†NG CH·ªÆ SANG S·ªê ---
    print("\n[PRE-PROCESSING] ƒêang x·ª≠ l√Ω d·ªØ li·ªáu d·∫°ng ch·ªØ...")

    # 1. X·ª≠ l√Ω B·ªánh m√£n t√≠nh (Chronic_Disease)
    # N·∫øu √¥ ch·ª©a ch·ªØ (kh√°c nan/tr·ªëng) -> 1 (C√≥ b·ªánh), ng∆∞·ª£c l·∫°i -> 0 (Kh√¥ng)
    if 'Chronic_Disease' in df.columns:
        df['Chronic_Disease'] = df['Chronic_Disease'].apply(
            lambda x: 0 if pd.isna(x) or str(x).strip().lower() in ['kh√¥ng', 'nan', '0'] else 1
        )
        print("-> ƒê√£ m√£ h√≥a 'Chronic_Disease' sang 0/1.")

    # 2. X·ª≠ l√Ω Th·ª±c ph·∫©m ch·ª©c nƒÉng & T·∫≠p th·ªÉ d·ª•c (Supp_..., Exercise)
    # Gi·∫£ ƒë·ªãnh d·ªØ li·ªáu l√† 'C√≥'/'Kh√¥ng' ho·∫∑c text -> chuy·ªÉn sang 1/0
    cols_to_binary = ['Supp_Calcium', 'Supp_Omega3', 'Exercise']
    for c in cols_to_binary:
        if c in df.columns:
            df[c] = df[c].apply(
                lambda x: 1 if str(x).strip().lower() in ['c√≥', 'yes', '1'] else 0
            )
            print(f"-> ƒê√£ m√£ h√≥a '{c}' sang 0/1.")

    # Ki·ªÉm tra l·∫°i xem c√≤n c·ªôt n√†o trong POTENTIAL_FEATS l√† d·∫°ng ch·ªØ kh√¥ng
    # N·∫øu c√≤n, √©p sang s·ªë ho·∫∑c lo·∫°i b·ªè
    valid_potential = []
    for c in POTENTIAL_FEATS:
        if c in df.columns:
            # √âp ki·ªÉu sang s·ªë, n·∫øu l·ªói bi·∫øn th√†nh NaN r·ªìi ƒëi·ªÅn 0
            df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0)
            valid_potential.append(c)

    print(f"‚úÖ ƒê√£ l√†m s·∫°ch s·ªë li·ªáu. S·ªë l∆∞·ª£ng bi·∫øn ti·ªÅm nƒÉng h·ª£p l·ªá: {len(valid_potential)}")

    # ==============================================================================
    # B∆Ø·ªöC 1: CLUSTERING & SPLIT DATA
    # ==============================================================================
    print("\n" + "="*80)
    print("B∆Ø·ªöC 1: PH√ÇN C·ª§M & CHIA T·∫¨P D·ªÆ LI·ªÜU (STRATIFIED SPLIT)")
    print("="*80)

    scaler = StandardScaler()
    X_cluster = scaler.fit_transform(df[DEMO_FEATS].fillna(0))

    kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
    df['Cluster'] = kmeans.fit_predict(X_cluster)

    print("\nüìä H·ªí S∆† ƒê·∫∂C TR∆ØNG C·ª¶A T·ª™NG NH√ìM (CLUSTER PROFILE):")
    profile = df.groupby('Cluster')[DEMO_FEATS].mean().T
    print(profile.round(2))
    print("-" * 60)
    print("-> GI·∫¢I TH√çCH NHANH:")
    for i in range(3):
        age = profile.loc['Age', i]
        income = profile.loc['Income_Code', i]
        print(f"   + Cluster {i}: ƒê·ªô tu·ªïi TB {age:.1f}, Thu nh·∫≠p TB m·ª©c {income:.1f}")

    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['Cluster'])
    print(f"\n‚úÖ ƒê√£ chia d·ªØ li·ªáu: Train ({len(train_df)}) - Test ({len(test_df)})")

    # ==============================================================================
    # B∆Ø·ªöC 2 & 3: T√åM 'C√ÇU H·ªéI V√ÄNG' RI√äNG BI·ªÜT CHO T·ª™NG NH√ìM
    # ==============================================================================
    print("\n" + "="*80)
    print("B∆Ø·ªöC 2 & 3: T√åM 'C√ÇU H·ªéI V√ÄNG' RI√äNG BI·ªÜT CHO T·ª™NG NH√ìM")
    print("="*80)

    targets = ['PSS_Score', 'MENQOL_Score']

    for target in targets:
        print(f"\n>>> ƒêANG X√ÇY D·ª∞NG CHI·∫æN L∆Ø·ª¢C CHO M·ª§C TI√äU: {target}")

        y_test_all = []
        y_pred_all = []

        for cluster_id in range(3):
            print(f"\n   [CLUSTER {cluster_id}] Ph√¢n t√≠ch ƒë·∫∑c tr∆∞ng...")

            c_train = train_df[train_df['Cluster'] == cluster_id]
            c_test = test_df[test_df['Cluster'] == cluster_id]

            if len(c_train) < 5:
                print("      ‚ö†Ô∏è Qu√° √≠t d·ªØ li·ªáu, b·ªè qua.")
                continue

            # Feature Selection
            selector = ExtraTreesRegressor(n_estimators=100, random_state=42)
            X_select = c_train[DEMO_FEATS + valid_potential].fillna(0)
            y_select = c_train[target]

            selector.fit(X_select, y_select)

            importances = pd.Series(selector.feature_importances_, index=X_select.columns)
            potential_only = importances[importances.index.isin(valid_potential)]
            top_5_questions = potential_only.nlargest(5).index.tolist()

            print(f"      -> 5 C√¢u h·ªèi v√†ng: {top_5_questions}")

            # ==========================================================================
            # B∆Ø·ªöC 4: FINE-TUNING
            # ==========================================================================
            final_features = DEMO_FEATS + top_5_questions
            X_train_fold = c_train[final_features].fillna(0)
            y_train_fold = c_train[target]

            # Tinh ch·ªânh nh·∫π nh√†ng ƒë·ªÉ tr√°nh overfit tr√™n t·∫≠p con
            param_grid = {
                'n_estimators': [100],
                'max_depth': [3, 5],
                'min_samples_leaf': [2, 4]
            }

            grid = GridSearchCV(ExtraTreesRegressor(random_state=42), param_grid, cv=3, n_jobs=-1)
            grid.fit(X_train_fold, y_train_fold)

            best_model = grid.best_estimator_
            # print(f"      -> Best Params: {grid.best_params_}") # T·∫Øt b·ªõt log cho g·ªçn

            # D·ª∞ B√ÅO
            X_test_fold = c_test[final_features].fillna(0)
            y_test_fold = c_test[target]
            y_pred = best_model.predict(X_test_fold)

            y_test_all.extend(y_test_fold)
            y_pred_all.extend(y_pred)

        # ==============================================================================
        # B∆Ø·ªöC 5: ƒê√ÅNH GI√Å
        # ==============================================================================
        print("\n" + "-"*60)
        print(f"K·∫æT QU·∫¢ CU·ªêI C√ôNG (M·ª§C TI√äU: {target})")
        r2_final = r2_score(y_test_all, y_pred_all)
        mae_final = mean_absolute_error(y_test_all, y_pred_all)

        print(f"‚úÖ R-Squared (ƒê·ªô ch√≠nh x√°c): {r2_final:.4f}")
        print(f"‚úÖ MAE (Sai s·ªë trung b√¨nh): {mae_final:.4f}")

        # V·∫Ω bi·ªÉu ƒë·ªì
        plt.figure(figsize=(6, 6))
        sns.scatterplot(x=y_test_all, y=y_pred_all, color='blue', alpha=0.6)
        plt.plot([min(y_test_all), max(y_test_all)], [min(y_test_all), max(y_test_all)], 'r--')
        plt.title(f'{target}: Th·ª±c t·∫ø vs D·ª± b√°o (Adaptive AI)')
        plt.xlabel('Th·ª±c t·∫ø')
        plt.ylabel('D·ª± b√°o')
        plt.tight_layout()
        plt.show()

if __name__ == "__main__":
    run_adaptive_pipeline()